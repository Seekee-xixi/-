## 5.14学习内容汇报_张雨溪

## **CHAP 1 线性代数基础知识**

### 本节目录

- [1. 线性代数基础知识](#1-线性代数基础知识)
  - [1.1 标量](#11-标量)
  - [1.2 向量](#12-向量)
  - [1.3 矩阵](#13-矩阵)
    - [1.3.1 矩阵的操作](#131-矩阵的操作)
    - [1.3.2 特殊矩阵](#132-特殊矩阵)
    - [1.3.3 特征向量和特征值](#133-特征向量和特征值)
- [2. 线性代数实现](#2-线性代数实现)
  - [2.1 标量](#21-标量)
  - [2.2 向量](#22-向量)
  - [2.3 矩阵](#23-矩阵)
    - [2.3.1 创建](#231-创建)
    - [2.3.2 转置](#232-转置)
    - [2.3.3 reshape](#233-reshape)
    - [2.3.4 clone](#234-clone)
    - [2.3.5 sum](#235-sum)
    - [2.3.6 numel](#236-numel)
    - [2.3.7 mean](#237-mean)
    - [2.3.8 dot](#238-dot)
    - [2.3.9 mm、mv](#239-mmmv)
    - [2.3.10  L1、L2、F范数](#2310--l1l2f范数)
    - [2.3.11  运算](#2311--运算)
    - [2.3.12 广播](#2312-广播)

### 1. 线性代数基础知识 ###

这部分主要是由标量过渡到向量，再从向量拓展到矩阵操作，重点在于理解矩阵层面上的操作（都是大学线代课的内容，熟悉的可以自动忽略）

#### 1.1 标量

#### 1.2 向量

#### 1.3 矩阵

##### 1.3.1 矩阵的操作

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-04.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-05.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-06.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-07.png)

	(矩阵范数麻烦且不常用，一般用F范数)

##### 1.3.2 特殊矩阵

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-08.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-09.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-10.png)

##### 1.3.3 特征向量和特征值

- 数学定义：设A是n阶方阵，如果存在常数![img](https://images0.cnblogs.com/blog/650633/201407/141700142243782.png)及非零n向量x，使得![img](https://images0.cnblogs.com/blog/650633/201407/141700145536982.png)，则称![img](https://images0.cnblogs.com/blog/650633/201407/141700149439396.png)是矩阵A的特征值，x是A属于特征值![img](https://images0.cnblogs.com/blog/650633/201407/141700153035353.png)的特征向量

- 直观理解：不被矩阵A改变方向的向量x就是A的一个特征向量

  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-11.png)

- 矩阵不一定有特征向量，但是对称矩阵总是可以找到特征向量



### 2. 线性代数实现

这部分主要是应用pytorch实现基本矩阵操作，同样由标量过渡到向量最后拓展到矩阵

#### 2.1 标量

```python
import torch    # 应用pytorch框架

# 标量由只有一个元素的张量表示
x = torch.tensor([3.0])     # 单独一个数字表示标量也可以
y = torch.tensor([2.0])     # 单独一个数字表示标量也可以
print(x + y)    # tensor([5.])
print(x * y)    # tensor([6.])
print(x / y)    # tensor([1.5000])
print(x ** y)   # tensor([9.]) 指数运算
```

#### 2.2 向量

```python
# 向量可以看作是若干标量值组成的列表
x = torch.arange(4)     # tensor([0, 1, 2, 3])
                        # 生成[0, 4)范围内所有整数构成的张量tensor
print(x[3])             # tensor(3)
                        # 和列表相似，通过张量的索引访问元素
print(len(x))           # 4
                        # 获取张量x的长度
print(x.shape)          # torch.Size([4])
                        # 获取张量形状，这里x是只有一个轴的张量因此形状只有一个元素

```

#### 2.3 矩阵

##### 2.3.1 创建

```python
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])
C = torch.tensor([[[1,2,3],
                   [4,5,6],
                   [7,8,9]],
                  [[0,0,0],
                   [1,1,1],
                   [2,2,2]]])
D = torch.arange(20, dtype=torch.float32)
```

##### 2.3.2 转置

```python
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
A = A.reshape(3,2)      # tensor([[0, 1],
                        #         [2, 3],
                        #         [4, 5]])

A = A.T                 # 转置 A.T
                        # tensor([[0, 2, 4],
                        #         [1, 3, 5]])
```

##### 2.3.3 reshape 

```python
# 使用reshape方法创建一个形状为3 x 2的矩阵A
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
A = A.reshape(3,2)      # tensor([[0, 1],
                        #         [2, 3],
                        #         [4, 5]])
```

*<u>tips（确定矩阵shape)：*</u>

<u>*由外层到内层依次去中括号，并记下去掉中括号后此时元素的个数，任选其中一个元素重复上述去括号的操作直到该元素中无中括号，记下的数字从左到右依次排序中间用x连接即为矩阵shape*</u>

##### 2.3.4 clone

```python
A = torch.arange(20, dtype=torch.float32)
A = A.reshape(5,4)
B = A.clone()   # 通过分配新内存，将A的一个副本分给B，该边B并不影响A的值
print(B)        # tensor([[ 0.,  1.,  2.,  3.],
                #         [ 4.,  5.,  6.,  7.],
                #         [ 8.,  9., 10., 11.],
                #         [12., 13., 14., 15.],
                #         [16., 17., 18., 19.]])
```

##### 2.3.5 sum

```python
A = torch.tensor([[[1,2,3],
                   [4,5,6],
                   [7,8,9]],
                  [[0,0,0],
                   [1,1,1],
                   [2,2,2]]])
print(A.shape)
# torch.Size([2, 3, 3])

print(A.sum())
# tensor(54)

print(A.sum(axis=0))
"""
tensor([[ 1,  2,  3],
        [ 5,  6,  7],
        [ 9, 10, 11]])
"""
print(A.sum(axis=0, keepdims=True))
"""
tensor([[[ 1,  2,  3],
         [ 5,  6,  7],
         [ 9, 10, 11]]])
"""

print(A.sum(axis=1))
"""
tensor([[12, 15, 18],
        [ 3,  3,  3]])
"""
print(A.sum(axis=1, keepdims=True))
"""
tensor([[[12, 15, 18]],

        [[ 3,  3,  3]]])
"""

print(A.sum(axis=2))
"""
tensor([[ 6, 15, 24],
        [ 0,  3,  6]])
"""
print(A.sum(axis=2, keepdims=True))
"""
tensor([[[ 6],
         [15],
         [24]],

        [[ 0],
         [ 3],
         [ 6]]])
"""

print(A.sum(axis=[0,1]))
# tensor([15, 18, 21])

print(A.sum(axis=[0,1], keepdims=True))
# tensor([[[15, 18, 21]]])
```

##### 2.3.6 numel

```python
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.numel())    # 6 元素个数
```

##### 2.3.7 mean

```python
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.numel())    # 6 元素个数
print(A.sum())      # tensor(3.)
print(A.mean())     # tensor(0.5000)

# 特定轴
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.shape[0])       # 2
print(A.sum(axis=0))    # tensor([1., 1., 1.])
print(A.mean(axis=0))   # tensor([0.5000, 0.5000, 0.5000])	平均值
```

##### 2.3.8 dot

```python
x = torch.tensor([0.,1.,2.,3.])
y = torch.tensor([1.,1.,1.,1.])
print(torch.dot(x, y))  # tensor(6.)
```

##### 2.3.9 mm、mv

```python
A = torch.tensor([[0,1,2],
                  [3,4,5]])
B = torch.tensor([[2,2],
                  [1,1],
                  [0,0]])
x = torch.tensor([3,3,3])

print(torch.mv(A, x))
"""
向量积
tensor([ 9, 36])
"""

print(torch.mm(A, B))
"""
矩阵积
tensor([[ 1,  1],
        [10, 10]])
"""
```

##### 2.3.10  L1、L2、F范数

```python
x = torch.tensor([3.0, -4.0])
print(torch.abs(x).sum())   # 向量的L1范数: tensor(7.)  x中的每个元素绝对值的和
print(torch.norm(x))        # 向量的L2范数: tensor(5.)  x中的每个元素平方的和开根号

A = torch.ones((4, 9))
print(torch.norm(A))        # 矩阵的F范数:  tensor(6.)  A中的每个元素平方的和开根号
```



##### 2.3.11  运算

```python
A = torch.arange(20, dtype=torch.float32)
A = A.reshape(5,4)
B = A.clone()   

print(B)        # tensor([[ 0.,  1.,  2.,  3.],
                #         [ 4.,  5.,  6.,  7.],
                #         [ 8.,  9., 10., 11.],
                #         [12., 13., 14., 15.],
                #         [16., 17., 18., 19.]])
                
print(A == B)
"""
tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
"""

print(A + B)
"""
tensor([[ 0.,  2.,  4.,  6.],
        [ 8., 10., 12., 14.],
        [16., 18., 20., 22.],
        [24., 26., 28., 30.],
        [32., 34., 36., 38.]])
"""


print(A * B)
"""
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
"""
```



##### 2.3.12 广播

```python
A = torch.tensor([[1.,2.,3.],
                  [4.,5.,6.]])
B = A.sum(axis=1, keepdims=True)

print(B)
"""
tensor([[ 6.],
        [15.]])
"""

print(A / B)
"""
tensor([[0.1667, 0.3333, 0.5000],
        [0.2667, 0.3333, 0.4000]])
"""

print(A + B)
"""
tensor([[ 7.,  8.,  9.],
        [19., 20., 21.]])
"""

print(A * B)
"""
tensor([[ 6., 12., 18.],
        [60., 75., 90.]])
"""
```

## **CHAP 2 矩阵计算**

### 本节目录

- [1. 导数的概念及几何意义](#1-导数的概念及几何意义)
  - [1.1 标量导数](#11-标量导数)
  - [1.2 亚导数](#12-亚导数)
- [2. 函数与标量，向量，矩阵](#2-函数与标量向量矩阵)
  - [2.1 f 为是一个标量](#21-f-为是一个标量)
    - [2.1.1 input是一个标量](#211-input是一个标量)
    - [2.1.2 input是一个向量](#212-input是一个向量)
    - [2.1.3 input是一个矩阵](#213-input是一个矩阵)
  - [2.2 f是一个向量](#22-f是一个向量)
    - [2.2.1 input是一个标量](#221-input是一个标量)
    - [2.2.2 input是一个向量](#222-input是一个向量)
    - [2.2.3 input是一个矩阵](#223-input是一个矩阵)
  - [2.3 F是一个矩阵](#23-f是一个矩阵)
    - [2.3.1 input是一个标量](#231-input是一个标量)
    - [2.3.2 input是一个向量](#232-input是一个向量)
    - [2.3.3 input是一个向量](#233-input是一个向量)
- [3. 求导的本质](#3-求导的本质)
- [4. 矩阵求导的布局](#4-矩阵求导的布局)
  - [4.1 分子布局](#41-分子布局)
  - [4.2 分母布局](#42-分母布局)

### 1. 导数的概念及几何意义

#### 1.1 标量导数

+ 导数是切线的斜率

+ 指向值变化最大的方向

#### 1.2 亚导数

+ 将导数拓展到不可微的函数，在不可导的点的导数可以用一个范围内的数表示

### 2. 函数与标量，向量，矩阵

该部分结合课程视频和参考文章进行总结（参考了知乎文章：[矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263777564)）

+ 当f，input为不同形式时，f(input)结果的表达形式

#### 2.1 f 是一个标量

##### 2.1.1 input是一个标量

##### 2.1.2 input是一个向量

##### 2.1.3 input是一个矩阵

#### 2.2 f是一个向量

+ **f**是由若干个f(标量)组成的向量

##### 2.2.1 input是一个标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-09.png)

##### 2.2.2 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-10.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-11.png)

##### 2.2.3 input是一个矩阵

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-12.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-13.png)

#### 2.3 F是一个矩阵

+ **F**是一个由若干**f**组成的一个矩阵

##### 2.3.1 input是一个标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-14.png)

##### 2.3.2 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-15.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-16.png)

##### 2.3.3 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-17.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-18.png)

### 3. 求导的本质

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-19.png)

**可以将f对x1，x2，x3的偏导分别求出来，即**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-20.png)

+ 矩阵求导也是一样的，**本质就是** ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bfunction%7D) 中的**每个** ![[公式]](https://www.zhihu.com/equation?tex=f) **分别对变元中的每个元素逐个求偏导，只不过写成了向量、矩阵形式而已。**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-21.png)

（课上是按行向量展开的）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-22.png)

**X为矩阵时**，先把矩阵变元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpmb%7BX%7D) 进行**转置**，再对**转置后**的**每个位置**的元素逐个求偏导，结果布局和**转置布局一样**。（课上讲的是这种展开方式)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-23.png)

+ 所以，如果 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bfunction%7D) 中有 ![[公式]](https://www.zhihu.com/equation?tex=m) 个 ![[公式]](https://www.zhihu.com/equation?tex=f) (标量)，变元中有 ![[公式]](https://www.zhihu.com/equation?tex=n) 个元素，那么，每个 ![[公式]](https://www.zhihu.com/equation?tex=f) 对变元中的每个元素逐个求偏导后，我们就会产生 ![[公式]](https://www.zhihu.com/equation?tex=m+%5Ctimes+n) 个结果。

### 4. 矩阵求导的布局

+ 经过上述对求导本质的推导，关于矩阵求导的问题，实质上就是对求导结果的进一步排布问题
  **对于2.2（f为向量，input也为向量）中的情况，其求导结果有两种排布方式，一种是`分子布局`，一种是`分母布局`**

##### 4.1 分子布局

**分子布局**，就是分子是**列向量**形式，分母是**行向量**形式 （课上讲的）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-24.png)

##### 4.2 分母布局

2.**分母布局**，就是分母是**列向量**形式，分子是**行向量**形式

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-25.png)

**将求导推广到矩阵，由于矩阵可以看作由多个向量所组成，因此对矩阵的求导可以看作先对每个向量进行求导，然后再增加一个维度存放求导结果。**

+  例如当F为矩阵，input为矩阵时，F中的每个元素f(标量）求导后均为一个矩阵（按照课上的展开方式），因此每个**f**（包含多个f（标量））求导后为存放多个矩阵的三维形状，再由于矩阵F由多个**f**组成，因此F求导后为存放多个**f**求导结果的四维形状。
   **对于不同f和input求导后的维度情况总结如下图所示（课程中的截图）**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-26.png)

## CHAP 3 链式法则与自动求导

### 本节目录

  - [1. 向量链式法则](#1-向量链式法则)
    - [例1（标量对向量求导）](#例1标量对向量求导)
    - [例2（涉及到矩阵的情况）](#例2涉及到矩阵的情况)
  - [2. 自动求导](#2-自动求导)
    - [2.1 计算图](#21-计算图)
    - [2.2 自动求导的两种模式](#22-自动求导的两种模式)
    - [2.3 复杂度比较](#23-复杂度比较)
  - [3. 代码部分](#3-代码部分)
  - [4. 自动求导 Q&A](#4-自动求导-qa)
  - [5. 练习](#5-练习)

### 1. 向量链式法则

- #### 1.1 标量链式法则

![image-20220107231957396](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-01)

- #### 1.2 拓展到向量

  > 需要注意维数的变化
  >
  > 下图三种情况分别对应：
  >
  > 1. y为标量，x为向量
  > 2. y为标量，x为矩阵
  > 3. y、x为矩阵

![image-20220107231931153](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-02)

---

#####         例1（标量对向量求导）

> 这里应该是用分子布局，所以是X转置

![image-20220107233527373](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-03)

#####          例2（涉及到矩阵的情况）

> X是mxn的矩阵,w为n维向量，y为m维向量；
> z对Xw-y做L2 norm,为标量；
> 过程与例一大体一致；
>
> ![image-20220107233753066](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-04)

---

> 由于在神经网络动辄几百层，手动进行链式求导是很困难的，因此需要借助自动求导

---

### 2. 自动求导

- 含义：计算一个函数在指定值上的导数

- 自动求导有别于

  - 符号求导

    ![image-20220107235547201](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-05)

  - 数值求导

    ![image-20220107235611767](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-06)

为了更好地理解自动求导，下面引入计算图的概念

#### 2.1 计算图

- 将代码分解成操作子

- 将计算表示成一个**无环图**

  > 下图自底向上其实就类似于链式求导过程

![image-20220107235918270](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-07)

​     

- 计算图有两种构造方式

  - 显示构造

    > 可以理解为先定义公式再代值
    >
    > Tensorflow/Theano/MXNet

    ![image-20220108000740736](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-08)

  - 隐式构造

    > 系统将所有的计算记录下来
    >
    > Pytorch/MXNet

    ![image-20220108001154208](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-09)

#### 2.2 自动求导的两种模式

- 正向累积

      ![image-20220108001506326](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-10)

- 反向累积（反向传递back propagation）![image-20220108001517029](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-11)

      **反向累积计算过程**

![image-20220108001847175](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-12)

> 反向累积的正向过程：自底向上，需要存储中间结果
>
> 反向累积的反向过程：自顶向下，可以去除不需要的枝（图中的x应为w）
>
> ![image-20220108002228166](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-13)

#### 2.3 复杂度比较

- 反向累积

  - 时间复杂度：O(n),n是操作子数
    - 通常正向和反向的代价类似
  - 空间复杂度：O(n)
    - 存储正向过程所有的中间结果

- 正向累积

  > 每次计算一个变量的梯度时都需要将所有节点扫一遍

  - 时间复杂度：O(n)
  - 空间复杂度：O(1)

---

### 3. 代码部分


```python
#对y = x.Tx关于列向量x求导
import torch

x = torch.arange(4.0)
x
```


    tensor([0., 1., 2., 3.])


```python
#存储梯度
x.requires_grad_(True)#等价于x = torch.arange(4.0,requires_grad=True)
x.grad#默认值是None
```


```python
y = torch.dot(x,x)
y
#PyTorch隐式地构造计算图，grad_fn用于记录梯度计算
```


    tensor(14., grad_fn=<DotBackward0>)

**通过调用反向传播函数来自动计算y关于x每个分量的梯度**


```python
y.backward()
x.grad
```


    tensor([0., 2., 4., 6.])


```python
x.grad==2*x#验证
```


    tensor([True, True, True, True])


```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()#如果没有这一步结果就会加累上之前的梯度值，变为[1,5,9,13]
y = x.sum()
y.backward()
x.grad
```


    tensor([1., 1., 1., 1.])


```python
x.grad.zero_()
y=x*x#哈达玛积，对应元素相乘

#在深度学习中我们一般不计算微分矩阵
#而是计算批量中每个样本单独计算的偏导数之和

y.sum().backward()#等价于y.backword(torch.ones(len(x)))
x.grad
```


    tensor([0., 2., 4., 6.])

**将某些计算移动到记录的计算图之外**


```python
# 后可用于用于将神经网络的一些参数固定住
x.grad.zero_()
y = x*x
u = y.detach()#把y当作常数
z = u*x

z.sum().backward()
x.grad == u
```


    tensor([True, True, True, True])


```python
x.grad.zero_()
y.sum().backward()
x.grad == 2*x
```


    tensor([True, True, True, True])

**即使构建函数的计算图需要用过Python控制流，仍然可以计算得到的变量的梯度**

**这也是隐式构造的优势，因为它会存储梯度计算的计算图，再次计算时执行反向过程就可以**


```python
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c

a = torch.randn(size=(),requires_grad=True)
d = f(a)
d.backward()

a.grad == d / a
```

### 4. 练习

**1.为什么计算二阶导数比一阶导数的开销要更大？**

二阶导数是在一阶导数的基础上进行的，开销自然更大

**2.在运行反向传播函数之后，立即再次运行它，看看会发生什么。**

"RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."

说明不能连续两次运行,pytorch使用的是动态计算图,反向传播函数运行一次后计算图就被释放了

**只需要在函数接口将参数retain_graph设为True即可**

In [51]:

```
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
a.grad.zero_()
a = torch.randn(size=(),requires_grad=True)#size=0表示a是标量
d = f(a)
#d.backward(retain_graph=True)
#a.grad
d.backward()
a.grad
```

Out[51]:

```
tensor(4096.)
```

**3.在控制流的例子中，我们计算d关于a的导数，如果我们将变量a更改为随机向量或矩阵，会发生什么？此时，计算结果f(a)不再是标量。结果会发生什么？我们如何分析这个结果？**

backward函数的机制本身不允许张量对张量求导，如果输入是向量或矩阵，需要将其在各个分量上求和，变为标量；所以还需要传入一个与输入同型的张量

In [53]:

```
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
a.grad.zero_()
a = torch.randn(10,requires_grad=True)
d = f(a)
#d.backward(retain_graph=True)
#a.grad
#d.backward()#RuntimeError: grad can be implicitly created only for scalar outputs
d.sum().backward()#需要加上.sum()否则会报错 
a.grad
```

Out[53]:

```
tensor([51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200.,
        51200.])
```

**4.重新设计一个求控制流梯度的例子。运行并分析结果。**

In [56]:

```
def h(x):
    y = x * x
    while y.norm() < 2500:
        y = y * 2
    if y.sum() < 0:
        c = 100*y
    else:
        c = y
    return c
x.grad.zero_()
x = torch.randn(size=(),requires_grad=True)
y = h(x)
y.backward()
x.grad
```

Out[56]:

```
tensor(-3311.5398)
```

**5.使f(x)=sin(x)，绘制f(x)和df(x)/dx的图像，其中后者不使用f'(x)=\cos(x)。**

In [66]:

```
import matplotlib.pyplot as plt
x = torch.arange(-20,20,0.1,requires_grad=True,dtype=torch.float32)
y = torch.sin(x)
y.sum().backward()
plt.plot(x.detach(),y.detach(),label='y=sinx')
plt.plot(x.detach(),x.grad,label='dy/dx')
plt.legend(loc='lower right')
```

Out[66]:

```
<matplotlib.legend.Legend at 0x1d627d00280>
```

![img.png](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-14)
